---
title: "Data-Science-Capstone"
author: "Qiang Wu"
date: "2016/3/19"
output: pdf_document
---

# Load Data
```{r, echo=TRUE, cache=TRUE}
Sys.setlocale(category = "LC_ALL", local="English_United States.1252")
library(tm)
library(ggplot2)
library(wordcloud)
blogs<-readLines("en_US.blogs.txt", encoding="UTF-8")
object.size(blogs)
twitter<-readLines("en_US.twitter.txt", encoding="UTF-8")
object.size(twitter)
news<-readLines("en_US.news.txt", encoding="UTF-8")
object.size(news)
```

# The data is too big, have to sample them. My laptop couldn't process the whole data. Also remove special non english character in files.
```{r, echo=TRUE, cache=TRUE}
twitter <- sample(twitter, size = round(length(twitter)/100))
twitter<-gsub("[^0-9A-Za-z///' ]", "", twitter)
blogs <- sample(blogs, size = round(length(blogs)/100))
blogs<-gsub("[^0-9A-Za-z///' ]", "", blogs)
news <- sample(news, size = round(length(news)/100))
news<-gsub("[^0-9A-Za-z///' ]", "", news)
object.size(twitter)
object.size(blogs)
object.size(news)
```

# Change data to Corpus
```{r, echo=TRUE, cache=TRUE}
news_corpus<-Corpus(VectorSource(news))
twitter_corpus<-Corpus(VectorSource(twitter))
blog_corpus<-Corpus(VectorSource(blogs))
meta(news_corpus)
meta(twitter_corpus)
meta(blog_corpus)
```

# Clean data. I notice that there is some not english character in twitter. Need to clean it before tolower.
```{r, echo=TRUE, cache=TRUE}
news_corpus<-tm_map(news_corpus, removePunctuation)
news_corpus<-tm_map(news_corpus,removeNumbers)
news_corpus<-tm_map(news_corpus,tolower)
news_corpus<-tm_map(news_corpus,removeWords,stopwords("english"))
news_corpus<-tm_map(news_corpus,stripWhitespace)
news_corpus<-tm_map(news_corpus, PlainTextDocument)

blog_corpus<-tm_map(blog_corpus, removePunctuation)
blog_corpus<-tm_map(blog_corpus,removeNumbers)
blog_corpus<-tm_map(blog_corpus,tolower)
blog_corpus<-tm_map(blog_corpus,removeWords,stopwords("english"))
blog_corpus<-tm_map(blog_corpus,stripWhitespace)
blog_corpus<-tm_map(blog_corpus, PlainTextDocument)

twitter_corpus<-tm_map(twitter_corpus, removePunctuation)
twitter_corpus<-tm_map(twitter_corpus,removeNumbers)
twitter_corpus<-tm_map(twitter_corpus,tolower)
twitter_corpus<-tm_map(twitter_corpus,removeWords,stopwords("english"))
twitter_corpus<-tm_map(twitter_corpus,stripWhitespace)
twitter_corpus<-tm_map(twitter_corpus, PlainTextDocument)
```

# Now create tokens, 2g/3g/4g tokens
## from http://tm.r-forge.r-project.org/faq.html
```{r, echo=TRUE, cache=TRUE}
all_corpus<-c(blog_corpus, twitter_corpus,news_corpus)
BigramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
TrigramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
QuadgramTokenizer <-  function(x) unlist(lapply(ngrams(words(x), 4), paste, collapse = " "), use.names = FALSE)
dtm <- DocumentTermMatrix(all_corpus)
#dtm2 <- DocumentTermMatrix(all_corpus, control = list(tokenize = BigramTokenizer))
#dtm3 <- DocumentTermMatrix(all_corpus, control = list(tokenize = TrigramTokenizer))
#dtm4 <- DocumentTermMatrix(all_corpus, control = list(tokenize = QuadgramTokenizer))
```

# Plot
```{r, echo=TRUE, cache=TRUE}
d<-removeSparseTerms(dtm,0.99)
freq<-sort (colSums (as.matrix(d)), decreasing =TRUE)
head(freq[1:20])
word_freq<-data.frame (word=names(freq), freq=freq)
ggplot(word_freq[1:10,],aes(x=reorder(word,freq),y=freq)) + geom_bar(stat="identity") +xlab("Word") + ylab("Frequency")
```

# Plot 2 tokens TBD
```{r, echo=TRUE, cache=TRUE}
#d2<-removeSparseTerms(dtm2,0.99)
#freq2<-sort (colSums (as.matrix(d2)), decreasing =TRUE)
#head(freq2[1:20])
#word_freq2<-data.frame (word=names(freq2), freq=freq2)
#ggplot(word_freq2[1:10,],aes(x=reorder(word,freq),y=freq)) + #geom_bar(stat="identity") +xlab("Word") + ylab("Frequency")
```

# Plot 3 tokens TBD
```{r, echo=TRUE, cache=TRUE}
#d3<-removeSparseTerms(dtm3,0.99)
#freq3<-sort (colSums (as.matrix(d3)), decreasing =TRUE)
#head(freq3[1:20])
#word_freq3<-data.frame (word=names(freq3), freq=freq3)
#ggplot(word_freq3[1:10,],aes(x=reorder(word,freq),y=freq)) + #geom_bar(stat="identity") +xlab("Word") + ylab("Frequency")
```

# Try to answer following questions:

## 1 Some words are more frequent than others - what are the distributions of word frequencies?
```{r, echo=TRUE, cache=TRUE}
quantile(freq)
```

## 2 What are the frequencies of 2-grams and 3-grams in the dataset? TBD
```{r, echo=TRUE, cache=TRUE}
#quantile(freq2)
#quantile(freq3)
```

## 3 How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? TBD

## 4 How do you evaluate how many of the words come from foreign languages? TBD

## 5 Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases? TBD

